"""
Audit Log Archiving Job
90 gÃ¼nden eski audit event kayÄ±tlarÄ±nÄ± arÅŸiv tablosuna taÅŸÄ±ma
"""

from apscheduler.triggers.cron import CronTrigger
from sqlalchemy.orm import Session
from datetime import datetime, timedelta
from sqlalchemy import text
import logging

logger = logging.getLogger(__name__)


async def audit_log_archive_job():
    """
    90 gÃ¼nden eski audit event'leri arÅŸiv tablosuna taÅŸÄ±
    
    Schedule: Her gÃ¼n 03:00 (dÃ¼ÅŸÃ¼k trafik saati)
    
    Workflow:
    1. 90 gÃ¼nden eski kayÄ±tlarÄ± seÃ§ (created_at < NOW() - INTERVAL '90 days')
    2. audit_events_archive tablosuna INSERT
    3. audit_events tablosundan DELETE
    4. VACUUM ANALYZE (PostgreSQL - disk space geri kazanÄ±m)
    5. Metrics: archived_records_total, archive_duration_seconds
    """
    from ..config.database import get_db
    
    db: Session = next(get_db())
    cutoff_date = datetime.utcnow() - timedelta(days=90)
    
    try:
        logger.info(f"ğŸ—‚ï¸  Audit log archiving baÅŸladÄ±. Cutoff date: {cutoff_date}")
        
        # Step 1: ArÅŸivlenecek kayÄ±t sayÄ±sÄ±nÄ± say
        # TODO: audit_events tablosu eklenince bu query Ã§alÄ±ÅŸacak
        # Åimdilik placeholder
        count_result = 0
        
        # count_result = db.execute(
        #     text("SELECT COUNT(*) FROM audit_events WHERE created_at < :cutoff"),
        #     {"cutoff": cutoff_date}
        # ).scalar()
        
        if count_result == 0:
            logger.info("âœ… ArÅŸivlenecek kayÄ±t yok")
            return
        
        logger.info(f"ğŸ“Š {count_result} kayÄ±t arÅŸivlenecek")
        
        # TODO: Batch processing implementation (FAZ 5 Phase 3)
        # batch_size = 1000
        # archived_count = 0
        
        # while True:
        #     result = db.execute(text("""
        #         WITH old_records AS (
        #             SELECT * FROM audit_events 
        #             WHERE created_at < :cutoff
        #             LIMIT :batch_size
        #         )
        #         INSERT INTO audit_events_archive 
        #         SELECT * FROM old_records
        #         RETURNING id
        #     """), {"cutoff": cutoff_date, "batch_size": batch_size})
        #     
        #     batch_count = result.rowcount
        #     if batch_count == 0:
        #         break
        #     
        #     archived_count += batch_count
        #     db.commit()
        
        logger.info(f"âœ… Audit log archiving baÅŸarÄ±lÄ± (placeholder)")
        
    except Exception as e:
        logger.error(f"âŒ Audit log archiving failed: {str(e)}", exc_info=True)
        db.rollback()
        raise
    finally:
        db.close()


def register_audit_archive_job(scheduler):
    """
    Audit archive job'Ä±nÄ± scheduler'a kaydet
    
    Args:
        scheduler: APScheduler instance
    """
    scheduler.add_job(
        audit_log_archive_job,
        trigger=CronTrigger(hour=3, minute=0, timezone='Europe/Istanbul'),
        id='audit_archive_daily',
        name='Audit Log Archiving (90+ days)',
        replace_existing=True
    )
    logger.info("ğŸ“‹ Audit archive job registered (daily at 03:00)")
